{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.optim as optim\n",
    "from torch import distributions as pyd\n",
    "import numpy as np\n",
    "# import environment as env_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta 0.05\n"
     ]
    }
   ],
   "source": [
    "# write a 6x6 tensor with half zeros and half ones randomly\n",
    "mdp_params = np.load(\"mdp/mdp_25.npz\")\n",
    "\n",
    "reward = (np.max(mdp_params['reward']) - mdp_params['reward']) / np.max(mdp_params['reward'])\n",
    "mu = mdp_params['mu']\n",
    "p_transition = mdp_params['p_transition'] # S * S' * A\n",
    "states = mdp_params['states']\n",
    "actions = mdp_params['n_actions']\n",
    "H = 100\n",
    "K = 10\n",
    "D = 50\n",
    "\n",
    "# eta = (H^2 * int(states) * int(actions) * K + H^4 * (K + D)) ** (-(1/2))\n",
    "eta = 0.05\n",
    "print('eta', eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'action_dim': int(actions),\n",
    "        'state_dim': int(states),\n",
    "        'transition_function': torch.from_numpy(p_transition),\n",
    "        'initial_state_distribution': torch.from_numpy(mu),\n",
    "        'reward_function': torch.from_numpy(reward),\n",
    "\n",
    "        'training horizon H': H,\n",
    "        'episodes K': K,\n",
    "        'eta': eta,\n",
    "        # 'gamma': 2 * eta * H,   \n",
    "        'gamma' : 0.005,\n",
    "\n",
    "        'max_delay': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAPO:\n",
    "    def __init__(self, config): \n",
    "        self.transition_function = config['transition_function']\n",
    "        self.initial_state_distribution = config['initial_state_distribution']\n",
    "        self.H = config[\"training horizon H\"]\n",
    "        self.K = config[\"episodes K\"]\n",
    "        self.reward_function = config['reward_function']\n",
    "        \n",
    "        # learning rate, exploration parameter\n",
    "        self.eta = config[\"eta\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "\n",
    "        # initialize space dimensions\n",
    "        self.A = config['action_dim']\n",
    "        self.S = config['state_dim']\n",
    "\n",
    "        # self.policy_history = torch.zeros(self.S, self.A, self.H, self.K)\n",
    "        self.policy_history = (1.0 / self.A) * torch.ones(self.S, self.A, self.H, self.K)\n",
    "\n",
    "        self.delay_dict = {}\n",
    "        for i in range(self.K): self.delay_dict[i] = []\n",
    "\n",
    "        self.traj_history = []\n",
    "\n",
    "        self.d = config['max_delay']\n",
    "\n",
    "    def sample_from_logits(self, logits):\n",
    "        sample = pyd.Categorical(logits=logits).sample()\n",
    "        return sample.item()\n",
    "\n",
    "    # WORKS\n",
    "    def play_one_step(self, current_state, policy, h):  \n",
    "        # Play one step in the environment\n",
    "        \n",
    "        # sample from policy at timestep h\n",
    "        action = self.sample_from_logits(policy[current_state, :, h])\n",
    "        # sample from transition to get next state\n",
    "        next_state = self.sample_from_logits(self.transition_function[:, current_state, action])\n",
    "        # get reward for state and action\n",
    "        reward = self.reward_function[current_state, action]\n",
    "        return action, next_state, reward\n",
    "\n",
    "    # WORKS\n",
    "    def play_episode(self, policy):\n",
    "        traj = []\n",
    "        # sample initial state\n",
    "        current_state = self.sample_from_logits(self.initial_state_distribution)\n",
    "        \n",
    "        # for 0, ..., H-1 play a step and set the current state to the next state\n",
    "        for h in range(self.H):\n",
    "            action, next_state, reward = self.play_one_step(current_state, policy, h)\n",
    "            traj.append((current_state, action, reward))\n",
    "            current_state = next_state\n",
    "        return traj\n",
    "    \n",
    "    # WORKS\n",
    "    def observe_feedback(self, traj):\n",
    "        # returns tensor of H * 1 rewards from a given trajectory\n",
    "        rewards = torch.zeros(self.H)\n",
    "        for h in range(self.H - 1):\n",
    "            s, a = traj[h][0], traj[h][1]\n",
    "            rewards[h] = self.reward_function[s, a]\n",
    "        return rewards\n",
    "\n",
    "\n",
    "    # rewrite the get_n_step trajectory\n",
    "    # def get_n_step_transition (self, h, k):\n",
    "    #     p = torch.zeros(self.S, self.S):\n",
    "    #     for i \n",
    "    \n",
    "    # WORKS (ish)\n",
    "    def get_one_step_transitions (self, k):\n",
    "        # returns a tensor of shape S * S * H which gives the one step transition probabilities\n",
    "        # from state i to j at each timestep h\n",
    "        p = torch.zeros(self.S, self.S, self.H)\n",
    "        for h in range(self.H):\n",
    "            for a in range(self.A):\n",
    "                p[:, :, h] += self.policy_history[:, a, h, k] * self.transition_function[:, :, a]\n",
    "        return p\n",
    "    \n",
    "    # WORKS (ish)\n",
    "    def get_occupancy(self, h, k):\n",
    "        # get the n-step transition probabilities\n",
    "        # p = self.get_one_step_transition(h, k)\n",
    "\n",
    "        p = self.get_one_step_transitions(k)\n",
    "\n",
    "        # occupancy measure is S * S\n",
    "\n",
    "        p_out = torch.zeros(self.S, self.S)\n",
    "        # p_out[:, :] = torch.ones(self.S, self.S)\n",
    "        p_out = p[:, :, 0]\n",
    "\n",
    "        for i in range(2, h+1):\n",
    "            for s in range(self.S):\n",
    "                p_out = p_out * p[:, :, i-1]\n",
    "\n",
    "        # # use dynamic programming to get the n-step transition probabilities\n",
    "        # for i in range(1, h+1):\n",
    "        #     for s in range(self.S):\n",
    "        #         # for s_p in range(self.S):\n",
    "        #         #     p_out[s, s_p, i] = p_out[s, s_p, i-1] * p[s, s_p, i-1]\n",
    "        #         p_out[:, :, i] += p_out[:, s, i-1] * p[:, :, i]\n",
    "\n",
    "        return p_out\n",
    "\n",
    "        # for i in range\n",
    "\n",
    "        # for i in range(1, h + 1):\n",
    "        #     p_h @= self.get_one_step_transition(i, k)\n",
    "        # p_h = torch.linalg.matrix_power(p, h)\n",
    "\n",
    "        # return p_h\n",
    "\n",
    "        # adjust for the fact that initial state is stochastic\n",
    "        # p_adj = torch.zeros(self.S)\n",
    "\n",
    "        # for s in range(self.S):\n",
    "        #     p_adj += self.initial_state_distribution[s] * p_out[s, :, h]\n",
    "\n",
    "        # return p_adj\n",
    "\n",
    "        # # p_adj represents the unconditional n-step transition probabilities\n",
    "        # # p_adj[i] is the probability of being in state i after h steps\n",
    "\n",
    "        # # occ_measure is a tensor of shape S * A\n",
    "        # occ_measure = p_adj.repeat(self.A, 1).T * self.policy_history[:, :, h, k]\n",
    "        \n",
    "        # # we want the sum over actions\n",
    "        # occ_measure = torch.sum(occ_measure, dim = 1)\n",
    "        # return p_adj, occ_measure\n",
    "\n",
    "    def unzip_trajectory(self, traj):\n",
    "        l = [list(t) for t in zip(*traj)]\n",
    "        return l[0], l[1], l[2]\n",
    "\n",
    "    def get_r(self, j_policy, k_policy):\n",
    "        # takes in two tensors of shape S * A * H\n",
    "        # outputs a tensor of shape S * A * H\n",
    "        return j_policy / torch.maximum(j_policy, k_policy)\n",
    "\n",
    "    def get_b(j_policy, k_policy, r, occupancies):\n",
    "        # outputs a tensor of shape S * H\n",
    "        output = torch.zeros(self.S, self.H)\n",
    "        for a in range(self.A):\n",
    "            output += (3 * self.gamma * self.H * k_policy[:, a, :] * r[:, a, :]) / (occupancies * j_policy[:, a, :] + self.gamma)\n",
    "\n",
    "    def train_one_step(self, k):\n",
    "        delay = np.random.choice(np.arange(k, k + self.d + 1))\n",
    "        self.delay_dict[delay].append(k)\n",
    "        delayed = self.delay_dict[k]\n",
    "\n",
    "        # play episode k with policy at k\n",
    "        k_trajectory = self.play_episode(self.policy_history[:,:,:,k])\n",
    "        self.traj_history.append(k_trajectory)\n",
    "        k_rewards = self.observe_feedback(k_trajectory)\n",
    "\n",
    "        # print the total rewards from episode k\n",
    "        print(f\"Episode: {k}, Total Reward: {torch.sum(k_rewards)}\")\n",
    "\n",
    "        # initialize Q and B\n",
    "        Q = torch.zeros(self.S, self.A, self.H, len(delayed))\n",
    "        B = torch.zeros(self.S, self.A, self.H + 1, len(delayed))\n",
    "\n",
    "        # the j are episode indexes for which we can now 'observe' the rewards\n",
    "        for j in delayed:\n",
    "            j_trajectory = self.traj_history[j]\n",
    "            j_rewards = self.observe_feedback(j_trajectory)\n",
    "\n",
    "            j_policy = self.policy_history[:, :, :, j]\n",
    "            k_policy = self.policy_history[:, :, :, k]\n",
    "            \n",
    "            # r is an S * A * H tensor\n",
    "            r = self.get_r(j_policy, k_policy)\n",
    "            # print('r', r)\n",
    "\n",
    "            # L is an H * 1 tensor\n",
    "            occupancies = torch.zeros(self.S, self.H)\n",
    "            L = torch.zeros(self.H)\n",
    "            # calculate occupanies as well, which is an S * H tensor\n",
    "            for h in range(self.H-1, -1, -1):\n",
    "                L[h] = torch.sum(j_rewards[h:])\n",
    "                occupancies[:, h] = self.get_occupancy(h, j)\n",
    "\n",
    "            print('occupancies', occupancies[:, 99])\n",
    "            print('occupancy sum', torch.sum(occupancies[:, 99]))\n",
    "            print('L', L)\n",
    "                \n",
    "            s_j, a_j, _ = self.unzip_trajectory(j_trajectory)\n",
    "\n",
    "            # Q is an S * A * H * J tensor\n",
    "            Q[s_j, a_j, :, j] = r[s_j, a_j] * L / (occupancies[s_j] * j_policy[s_j, a_j] + self.gamma)\n",
    "\n",
    "            print('Q', Q[:, :, 0, 0])\n",
    "\n",
    "            # b is an S * H tensor\n",
    "            b = torch.zeros(self.S, self.H)\n",
    "            for a in range(self.A):\n",
    "                # print('k_policy shape', k_policy[:, a, :].shape)\n",
    "                # print('r shape', r.shape)\n",
    "                # print('occupancies shape', occupancies.shape)\n",
    "                # print('j_policy shape', j_policy.shape)\n",
    "                # print('x shape', x.shape)\n",
    "                b += (3 * self.gamma * self.H * k_policy[:, a, :] * r[:, a, :]) / (occupancies * j_policy[:, a, :] + self.gamma)\n",
    "\n",
    "            print('b at H-2', b[:, self.H-2])\n",
    "            print('b at H-3', b[:, self.H-3])\n",
    "            print('b at 0', b[:, 0])\n",
    "            # now we calculate B which is S * A * H * J\n",
    "\n",
    "            for h in range(self.H-2, -1, -1):\n",
    "                # Now we calculate B for S * A in time h and for episode j\n",
    "\n",
    "                # B[:, :, h, j] = b[:, h].repeat(self.A, 1).T  \n",
    "                inner_sum = torch.zeros(self.S, self.A)\n",
    "                for s in range(self.S):\n",
    "                    for a in range(self.A):\n",
    "                        # print('self.transition_function shape', self.transition_function[s, : ,a].repeat(self.A, 1).T.shape)\n",
    "                        # print('j_policy shape', self.policy_history[:, :, h+1, j].shape)\n",
    "                        # print('B shape', B[:, :, h+1, j].shape)\n",
    "                        inner_sum += self.transition_function[s, : ,a].repeat(self.A, 1).T * self.policy_history[:, :, h+1, j] * B[:, :, h+1, j]\n",
    "                B[:, :, h, j] = b[:, h].repeat(self.A, 1).T + inner_sum\n",
    "\n",
    "                # Now we update the policy\n",
    "\n",
    "        # this is not right\n",
    "        print('B at H-1', B[:, :, self.H-1, 0])\n",
    "        print('B at H-2', B[:, :, self.H-2, 0])\n",
    "        print('B at H-3', B[:, :, self.H-3, 0])\n",
    "\n",
    "        for h in range(self.H):\n",
    "            for s in range(self.S):\n",
    "                for a in range(self.A):\n",
    "                    \n",
    "                    num_sum = 0.\n",
    "                    for j in delayed:\n",
    "                        num_sum += Q[s, a, h, j] - B[s, a, h, j]\n",
    "\n",
    "                    print('num_sum', num_sum)\n",
    "                    numerator = self.policy_history[s, a, h, k] * np.exp(-1 * self.eta * num_sum)\n",
    "\n",
    "                    denominator = 0.\n",
    "\n",
    "                    for a_prime in range(self.A):\n",
    "                        inner_denom_sum = 0.\n",
    "                        for j in delayed:\n",
    "                            inner_sum += Q[s, a_prime, h, j] - B[s, a_prime, h, j]\n",
    "                        denominator += self.policy_history[s, a_prime, h, k] * np.exp(-1 * self.eta * inner_sum)\n",
    "\n",
    "                    print('numerator', numerator)\n",
    "                    print('denominator', denominator)\n",
    "                    \n",
    "                    if k != (self.K - 1): # do not update policy at last episode\n",
    "                        # self.policy_history[s][a][h][k + 1] = numerator / (denominator if denominator != 0 else 1)\n",
    "                        self.policy_history[s][a][h][k + 1] = numerator / denominator\n",
    "\n",
    "    def train(self):\n",
    "        for k in range(self.K):\n",
    "            self.train_one_step(k)\n",
    "        print(self.delay_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAPO_test = DAPO(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00])"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAPO_test.get_occupancy(99, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.7497e-01, 8.3333e-04, 8.3333e-04, 8.5338e-02, 8.3333e-04, 8.3333e-04,\n",
       "         8.3333e-04, 8.3397e-04, 8.4658e-02, 8.3333e-04, 8.3333e-04, 9.2366e-04,\n",
       "         8.3333e-04, 8.3341e-04, 8.3333e-04, 8.3333e-04, 2.0086e-02, 8.3333e-04,\n",
       "         1.4896e-03, 8.3333e-04, 8.3333e-04, 3.4823e-03, 8.3333e-04, 8.4229e-02,\n",
       "         3.1488e-02],\n",
       "        [6.6758e-03, 6.1377e-01, 6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.7485e-03, 6.6667e-03, 6.6667e-03, 9.1737e-02,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 1.0142e-02, 6.6679e-03, 6.6667e-03,\n",
       "         4.8768e-02, 6.6667e-03, 6.6667e-03, 9.6945e-02, 6.6667e-03, 1.1875e-02,\n",
       "         6.6667e-03],\n",
       "        [6.6828e-03, 6.6667e-03, 7.9442e-01, 6.6667e-03, 6.6667e-03, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03, 1.0736e-02,\n",
       "         6.6667e-03, 6.6673e-03, 6.6667e-03, 6.6667e-03, 1.9488e-02, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 9.2708e-03, 6.6667e-03, 3.2739e-02,\n",
       "         6.6667e-03],\n",
       "        [8.3069e-03, 8.3333e-04, 8.3333e-04, 3.2838e-01, 8.3333e-04, 8.3333e-04,\n",
       "         8.3333e-04, 8.3333e-04, 1.4495e-01, 8.3333e-04, 8.3333e-04, 1.4182e-01,\n",
       "         8.3333e-04, 8.4167e-02, 8.3333e-04, 8.3333e-04, 2.1846e-02, 8.3333e-04,\n",
       "         1.8432e-02, 8.3333e-04, 8.4167e-02, 5.6473e-02, 8.3333e-04, 9.8957e-02,\n",
       "         8.4012e-04],\n",
       "        [6.6820e-03, 6.6667e-03, 6.6667e-03, 6.6741e-03, 6.5473e-01, 6.6667e-03,\n",
       "         6.6667e-03, 7.9894e-03, 1.1875e-02, 6.6667e-03, 5.5287e-02, 9.0000e-02,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 1.1717e-02, 1.7106e-02, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 1.7083e-02, 6.6667e-03, 2.7522e-02,\n",
       "         6.6667e-03],\n",
       "        [7.5833e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 5.5000e-02, 3.4167e-02,\n",
       "         3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 1.1750e-01,\n",
       "         3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02,\n",
       "         3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02,\n",
       "         3.4167e-02],\n",
       "        [6.7231e-03, 6.9922e-03, 6.6667e-03, 1.1956e-02, 6.6667e-03, 6.6667e-03,\n",
       "         7.7174e-01, 6.6667e-03, 6.6667e-03, 6.7218e-03, 6.6667e-03, 2.7500e-02,\n",
       "         6.6667e-03, 6.6830e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6775e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 4.8343e-02, 6.6667e-03, 6.6668e-03,\n",
       "         6.6667e-03],\n",
       "        [1.4733e-01, 6.6667e-03, 6.6667e-03, 9.2930e-02, 6.6959e-03, 6.6667e-03,\n",
       "         6.6667e-03, 4.7948e-01, 2.5921e-02, 6.6667e-03, 6.6667e-03, 6.7548e-03,\n",
       "         6.6667e-03, 5.4292e-02, 6.6667e-03, 6.6667e-03, 2.9817e-02, 6.6667e-03,\n",
       "         6.7390e-03, 6.6667e-03, 6.6667e-03, 4.9039e-02, 6.6667e-03, 6.6897e-03,\n",
       "         7.6464e-03],\n",
       "        [9.3578e-02, 3.3333e-03, 3.3333e-03, 9.9777e-03, 3.3333e-03, 3.3333e-03,\n",
       "         3.3333e-03, 4.6354e-03, 1.9335e-01, 3.3333e-03, 3.3333e-03, 8.1865e-02,\n",
       "         3.3333e-03, 3.7300e-03, 3.3333e-03, 3.3333e-03, 3.3558e-03, 3.3333e-03,\n",
       "         7.2780e-02, 3.3333e-03, 3.3333e-03, 1.0750e-01, 3.3333e-03, 2.5341e-01,\n",
       "         1.2915e-01],\n",
       "        [6.7205e-03, 6.6798e-03, 6.6798e-03, 1.3624e-02, 6.6798e-03, 6.6798e-03,\n",
       "         6.6798e-03, 6.6798e-03, 3.4458e-02, 6.5249e-01, 6.6798e-03, 9.2841e-03,\n",
       "         6.6798e-03, 6.2235e-02, 6.6798e-03, 6.6798e-03, 1.3709e-02, 6.6798e-03,\n",
       "         6.6798e-03, 6.6798e-03, 6.6798e-03, 6.6798e-03, 6.6798e-03, 9.2840e-03,\n",
       "         9.1315e-02],\n",
       "        [9.1471e-04, 8.3333e-04, 8.3333e-04, 8.3341e-04, 4.5455e-03, 8.3333e-04,\n",
       "         8.3333e-04, 4.3462e-03, 8.4789e-04, 8.3333e-04, 7.0098e-01, 1.1251e-02,\n",
       "         8.3333e-04, 8.3333e-04, 8.3333e-04, 5.3418e-02, 8.4167e-02, 8.3333e-04,\n",
       "         8.3333e-04, 8.3333e-04, 8.3333e-04, 8.3333e-04, 8.3333e-04, 4.2539e-02,\n",
       "         8.4495e-02],\n",
       "        [1.0692e-02, 5.2083e-05, 8.3385e-02, 1.6722e-02, 5.2083e-05, 5.2083e-05,\n",
       "         5.2083e-05, 5.2083e-05, 4.1729e-02, 5.2083e-05, 5.2084e-05, 7.4013e-01,\n",
       "         5.2083e-05, 4.8270e-02, 5.2083e-05, 7.0628e-04, 1.3548e-03, 6.2256e-05,\n",
       "         2.3523e-02, 5.2083e-05, 5.2083e-05, 2.1759e-04, 5.2083e-05, 2.1548e-02,\n",
       "         1.1033e-02],\n",
       "        [6.6667e-03, 2.7500e-02, 6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03,\n",
       "         6.6667e-03, 6.6672e-03, 6.9473e-03, 6.6667e-03, 6.6667e-03, 6.7390e-03,\n",
       "         7.7433e-01, 9.2709e-03, 6.6667e-03, 6.6667e-03, 4.8334e-02, 6.6667e-03,\n",
       "         6.6668e-03, 6.6667e-03, 6.6667e-03, 6.8750e-03, 6.6667e-03, 6.6667e-03,\n",
       "         6.6730e-03],\n",
       "        [6.9923e-03, 6.6667e-03, 6.6667e-03, 4.9373e-02, 6.6667e-03, 6.6667e-03,\n",
       "         6.6667e-03, 1.0928e-02, 1.7140e-02, 6.6667e-03, 6.6667e-03, 1.1896e-02,\n",
       "         6.6667e-03, 4.7128e-01, 6.6667e-03, 6.6667e-03, 2.7587e-02, 6.6667e-03,\n",
       "         4.8361e-02, 6.6667e-03, 6.6667e-03, 1.7095e-02, 6.6667e-03, 2.2312e-02,\n",
       "         2.2371e-01],\n",
       "        [3.1949e-02, 2.5005e-02, 2.5005e-02, 2.7088e-02, 2.5005e-02, 2.5005e-02,\n",
       "         2.5005e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02, 2.8477e-02,\n",
       "         2.5005e-02, 1.0834e-01, 1.5811e-01, 1.0834e-01, 2.5005e-02, 2.5005e-02,\n",
       "         6.6671e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02,\n",
       "         4.5954e-02],\n",
       "        [2.7500e-02, 6.6667e-03, 6.6667e-03, 6.8294e-03, 5.4065e-02, 6.6667e-03,\n",
       "         6.6667e-03, 6.6668e-03, 1.3611e-02, 6.6667e-03, 1.7191e-02, 2.1478e-02,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 7.2431e-01, 6.6680e-03, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6683e-03, 6.6667e-03, 7.7939e-03,\n",
       "         2.0556e-02],\n",
       "        [2.3092e-02, 8.3333e-04, 8.3333e-04, 1.7976e-02, 8.3333e-04, 8.3333e-04,\n",
       "         8.3333e-04, 4.2950e-02, 5.6942e-02, 8.3333e-04, 8.3333e-04, 3.4167e-02,\n",
       "         8.3333e-04, 7.7056e-03, 8.3333e-04, 4.2500e-02, 6.4362e-01, 8.3333e-04,\n",
       "         8.5210e-04, 8.3333e-04, 8.3333e-04, 2.7637e-02, 8.3333e-04, 9.0719e-02,\n",
       "         1.0012e-03],\n",
       "        [3.3740e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03,\n",
       "         3.3333e-03, 3.3333e-03, 2.4188e-02, 3.3333e-03, 3.3333e-03, 1.3203e-01,\n",
       "         3.3333e-03, 3.3337e-03, 3.3333e-03, 3.3333e-03, 3.5529e-03, 7.6930e-01,\n",
       "         3.4961e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03, 3.9845e-03,\n",
       "         3.4159e-03],\n",
       "        [3.5401e-03, 3.3333e-03, 3.3333e-03, 1.6683e-01, 3.3333e-03, 3.3333e-03,\n",
       "         3.3333e-03, 3.3333e-03, 4.9617e-02, 3.3333e-03, 3.3333e-03, 2.2279e-02,\n",
       "         3.3333e-03, 1.1256e-02, 3.3333e-03, 3.4249e-03, 9.7101e-02, 3.3333e-03,\n",
       "         3.8351e-01, 3.3333e-03, 3.3333e-03, 3.3334e-03, 3.3333e-03, 1.1405e-01,\n",
       "         9.8391e-02],\n",
       "        [9.0462e-02, 6.6927e-03, 6.6927e-03, 7.3193e-03, 6.7010e-03, 6.6927e-03,\n",
       "         6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6927e-03, 9.0067e-02,\n",
       "         6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6931e-03, 6.6927e-03,\n",
       "         6.6927e-03, 6.7160e-01, 6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6941e-03,\n",
       "         6.6927e-03],\n",
       "        [6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03, 1.7083e-02, 6.6667e-03,\n",
       "         6.6667e-03, 6.6802e-03, 4.8333e-02, 6.6667e-03, 4.8333e-02, 6.6673e-03,\n",
       "         6.6667e-03, 2.7502e-02, 6.6667e-03, 6.6802e-03, 1.7083e-02, 6.9239e-03,\n",
       "         7.9710e-03, 6.6667e-03, 6.9192e-01, 6.6667e-03, 6.6667e-03, 6.6689e-03,\n",
       "         2.8157e-02],\n",
       "        [5.2399e-02, 3.3333e-03, 3.3333e-03, 5.8976e-02, 8.9092e-03, 3.3333e-03,\n",
       "         3.3333e-03, 9.6541e-03, 3.3874e-03, 3.3333e-03, 3.3333e-03, 9.7083e-02,\n",
       "         3.3333e-03, 3.3333e-03, 3.3333e-03, 3.3435e-03, 1.0494e-02, 3.3333e-03,\n",
       "         3.5371e-03, 3.3333e-03, 3.3333e-03, 6.6965e-01, 3.3333e-03, 3.5897e-02,\n",
       "         3.3334e-03],\n",
       "        [6.6699e-03, 6.6699e-03, 6.6699e-03, 6.6722e-03, 6.6699e-03, 6.6699e-03,\n",
       "         6.6699e-03, 6.6699e-03, 6.7106e-03, 6.6699e-03, 6.6699e-03, 6.6699e-03,\n",
       "         6.6699e-03, 6.6699e-03, 6.6699e-03, 6.6699e-03, 6.6708e-03, 6.6699e-03,\n",
       "         6.6699e-03, 6.6699e-03, 6.6699e-03, 6.6699e-03, 8.3988e-01, 6.6711e-03,\n",
       "         6.6699e-03],\n",
       "        [2.6601e-01, 5.2083e-05, 5.2083e-05, 8.5644e-03, 5.2083e-05, 5.2083e-05,\n",
       "         5.2083e-05, 8.5094e-02, 2.9622e-04, 5.2083e-05, 4.1719e-02, 3.9997e-02,\n",
       "         5.2083e-05, 1.3073e-02, 5.2083e-05, 1.3941e-02, 5.2084e-05, 5.2088e-05,\n",
       "         7.8451e-04, 5.2083e-05, 5.2083e-05, 1.1793e-04, 5.2083e-05, 5.2967e-01,\n",
       "         5.2085e-05],\n",
       "        [1.2519e-02, 1.6667e-03, 1.6667e-03, 1.4067e-02, 1.6667e-03, 1.6667e-03,\n",
       "         1.6667e-03, 2.3177e-03, 5.8325e-02, 1.6667e-03, 4.2708e-03, 5.6049e-02,\n",
       "         1.6667e-03, 2.1617e-01, 1.6667e-03, 1.6667e-03, 9.3584e-03, 1.6787e-03,\n",
       "         2.2682e-02, 1.6667e-03, 1.6667e-03, 1.6667e-03, 1.6667e-03, 2.9942e-03,\n",
       "         5.7790e-01]])"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAPO_test.get_one_step_transitions(0)[:, :, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1279, 0.0046, 0.0124, 0.0747, 0.0176, 0.0026, 0.0060, 0.0372, 0.0635,\n",
       "        0.0043, 0.0167, 0.1032, 0.0061, 0.0765, 0.0026, 0.0225, 0.0720, 0.0083,\n",
       "        0.0438, 0.0046, 0.0143, 0.0683, 0.0056, 0.1142, 0.0906])"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAPO_test.get_occupancy(99, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 77.35842895507812\n",
      "occupancies tensor([0.1279, 0.0046, 0.0124, 0.0747, 0.0176, 0.0026, 0.0060, 0.0372, 0.0635,\n",
      "        0.0043, 0.0167, 0.1032, 0.0061, 0.0765, 0.0026, 0.0225, 0.0720, 0.0083,\n",
      "        0.0438, 0.0046, 0.0143, 0.0683, 0.0056, 0.1142, 0.0906])\n",
      "occupancy sum tensor(1.0000)\n",
      "L tensor([77.3584, 76.3754, 75.3947, 74.5676, 73.5869, 73.4024, 73.4024, 73.4024,\n",
      "        72.4217, 71.4383, 70.4625, 69.8617, 68.8793, 68.4598, 67.4774, 67.4774,\n",
      "        66.5051, 65.5226, 64.5392, 63.5557, 62.5732, 61.5909, 61.5909, 60.6075,\n",
      "        59.7804, 59.5665, 58.6782, 58.4937, 57.6054, 56.6218, 55.7335, 54.9323,\n",
      "        53.9500, 52.9742, 51.9907, 51.7767, 50.7943, 50.1935, 49.2100, 48.2377,\n",
      "        47.2542, 46.4529, 45.4705, 44.4875, 43.5045, 42.5220, 41.5395, 40.5561,\n",
      "        39.5727, 38.7457, 37.9016, 36.9293, 35.9535, 35.5340, 34.5510, 33.7239,\n",
      "        32.7415, 31.7762, 30.7938, 29.8108, 28.8284, 27.8451, 26.8972, 26.0864,\n",
      "        25.8725, 25.6880, 25.5034, 25.2895, 24.3242, 23.3407, 22.3573, 21.3850,\n",
      "        20.4371, 19.6358, 19.4219, 18.4566, 17.4914, 16.6031, 16.6031, 16.3892,\n",
      "        15.5879, 14.7608, 13.7778, 12.8125, 11.8473, 10.8643,  9.8813,  9.8813,\n",
      "         9.0800,  8.1042,  7.9197,  6.9361,  6.1348,  5.3241,  4.3416,  3.3592,\n",
      "         2.5484,  1.5831,  0.9824,  0.0000])\n",
      "Q tensor([[    0.0000,     0.0000,  2939.5754,     0.0000,  2939.5754,     0.0000],\n",
      "        [    0.0000, 13418.0771, 13418.0771, 13418.0771, 13418.0771,     0.0000],\n",
      "        [10955.0557,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,  4431.2217,  4431.2217,  4431.2217,  4431.2217,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,  9745.1475,  9745.1475,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [12907.8066, 12907.8066,     0.0000, 12907.8066, 12907.8066, 12907.8066],\n",
      "        [    0.0000,  6902.3047,     0.0000,     0.0000,     0.0000,  6902.3047],\n",
      "        [ 4962.9629,  4962.9629,  4962.9629,  4962.9629,     0.0000,     0.0000],\n",
      "        [    0.0000, 13527.8984, 13527.8984, 13527.8984, 13527.8984, 13527.8984],\n",
      "        [    0.0000,  9943.8477,  9943.8477,  9943.8477,     0.0000,  9943.8477],\n",
      "        [ 3483.5337,  3483.5337,     0.0000,  3483.5337,  3483.5337,  3483.5337],\n",
      "        [    0.0000,     0.0000,     0.0000, 12857.0947,     0.0000, 12857.0947],\n",
      "        [ 4358.8823,     0.0000,     0.0000,  4358.8823,  4358.8823,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [ 8848.7236,  8848.7236,     0.0000,  8848.7236,  8848.7236,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,  4550.1206],\n",
      "        [    0.0000, 12133.3711, 12133.3711,     0.0000,     0.0000, 12133.3711],\n",
      "        [ 6289.2119,  6289.2119,     0.0000,  6289.2119,  6289.2119,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000, 13428.1943, 13428.1943],\n",
      "        [10468.8555, 10468.8555,     0.0000, 10468.8555,     0.0000,     0.0000],\n",
      "        [    0.0000,  4720.7065,     0.0000,     0.0000,     0.0000,  4720.7065],\n",
      "        [13054.5234,     0.0000,     0.0000,     0.0000, 13054.5234,     0.0000],\n",
      "        [ 3218.8557,     0.0000,  3218.8557,     0.0000,  3218.8557,     0.0000],\n",
      "        [ 3848.1594,  3848.1594,     0.0000,  3848.1594,     0.0000,     0.0000]])\n",
      "b at H-2 tensor([ 56.9989, 260.1799, 212.4213,  85.9223, 188.9607, 276.4433, 250.2856,\n",
      "        133.8372,  96.2329, 262.3094, 192.8136,  67.5464, 249.3023,  84.5196,\n",
      "        276.4049, 171.5788,  88.2278, 235.2691, 121.9492, 260.3761, 202.9936,\n",
      "         91.5355, 253.1305,  62.4142,  74.6166])\n",
      "b at H-3 tensor([ 56.9989, 260.1799, 212.4213,  85.9223, 188.9607, 276.4433, 250.2856,\n",
      "        133.8372,  96.2329, 262.3094, 192.8136,  67.5464, 249.3023,  84.5196,\n",
      "        276.4049, 171.5788,  88.2278, 235.2691, 121.9492, 260.3761, 202.9936,\n",
      "         91.5355, 253.1305,  62.4142,  74.6166])\n",
      "b at 0 tensor([ 56.9991, 260.1800, 212.4214,  85.9225, 188.9609, 276.4433, 250.2857,\n",
      "        133.8375,  96.2331, 262.3094, 192.8138,  67.5466, 249.3024,  84.5199,\n",
      "        276.4050, 171.5791,  88.2280, 235.2692, 121.9494, 260.3762, 202.9938,\n",
      "         91.5357, 253.1306,  62.4144,  74.6168])\n",
      "B at H-1 tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "B at H-2 tensor([[ 56.9989,  56.9989,  56.9989,  56.9989,  56.9989,  56.9989],\n",
      "        [260.1799, 260.1799, 260.1799, 260.1799, 260.1799, 260.1799],\n",
      "        [212.4213, 212.4213, 212.4213, 212.4213, 212.4213, 212.4213],\n",
      "        [ 85.9223,  85.9223,  85.9223,  85.9223,  85.9223,  85.9223],\n",
      "        [188.9607, 188.9607, 188.9607, 188.9607, 188.9607, 188.9607],\n",
      "        [276.4433, 276.4433, 276.4433, 276.4433, 276.4433, 276.4433],\n",
      "        [250.2856, 250.2856, 250.2856, 250.2856, 250.2856, 250.2856],\n",
      "        [133.8372, 133.8372, 133.8372, 133.8372, 133.8372, 133.8372],\n",
      "        [ 96.2329,  96.2329,  96.2329,  96.2329,  96.2329,  96.2329],\n",
      "        [262.3094, 262.3094, 262.3094, 262.3094, 262.3094, 262.3094],\n",
      "        [192.8136, 192.8136, 192.8136, 192.8136, 192.8136, 192.8136],\n",
      "        [ 67.5464,  67.5464,  67.5464,  67.5464,  67.5464,  67.5464],\n",
      "        [249.3023, 249.3023, 249.3023, 249.3023, 249.3023, 249.3023],\n",
      "        [ 84.5196,  84.5196,  84.5196,  84.5196,  84.5196,  84.5196],\n",
      "        [276.4049, 276.4049, 276.4049, 276.4049, 276.4049, 276.4049],\n",
      "        [171.5788, 171.5788, 171.5788, 171.5788, 171.5788, 171.5788],\n",
      "        [ 88.2278,  88.2278,  88.2278,  88.2278,  88.2278,  88.2278],\n",
      "        [235.2691, 235.2691, 235.2691, 235.2691, 235.2691, 235.2691],\n",
      "        [121.9492, 121.9492, 121.9492, 121.9492, 121.9492, 121.9492],\n",
      "        [260.3761, 260.3761, 260.3761, 260.3761, 260.3761, 260.3761],\n",
      "        [202.9936, 202.9936, 202.9936, 202.9936, 202.9936, 202.9936],\n",
      "        [ 91.5355,  91.5355,  91.5355,  91.5355,  91.5355,  91.5355],\n",
      "        [253.1305, 253.1305, 253.1305, 253.1305, 253.1305, 253.1305],\n",
      "        [ 62.4142,  62.4142,  62.4142,  62.4142,  62.4142,  62.4142],\n",
      "        [ 74.6166,  74.6166,  74.6166,  74.6166,  74.6166,  74.6166]])\n",
      "B at H-3 tensor([[147.2257, 147.2257, 147.2257, 147.2257, 147.2257, 147.2257],\n",
      "        [464.6596, 464.6596, 464.6596, 464.6596, 464.6596, 464.6596],\n",
      "        [430.9463, 430.9463, 430.9463, 430.9463, 430.9463, 430.9463],\n",
      "        [171.3544, 171.3544, 171.3544, 171.3544, 171.3544, 171.3544],\n",
      "        [357.8329, 357.8329, 357.8329, 357.8329, 357.8329, 357.8329],\n",
      "        [320.0249, 320.0249, 320.0249, 320.0249, 320.0249, 320.0249],\n",
      "        [481.2292, 481.2292, 481.2292, 481.2292, 481.2292, 481.2292],\n",
      "        [237.5619, 237.5619, 237.5619, 237.5619, 237.5619, 237.5619],\n",
      "        [183.7310, 183.7310, 183.7310, 183.7310, 183.7310, 183.7310],\n",
      "        [473.0799, 473.0799, 473.0799, 473.0799, 473.0799, 473.0799],\n",
      "        [386.1816, 386.1816, 386.1816, 386.1816, 386.1816, 386.1816],\n",
      "        [194.7429, 194.7429, 194.7429, 194.7429, 194.7429, 194.7429],\n",
      "        [479.9846, 479.9846, 479.9846, 479.9846, 479.9846, 479.9846],\n",
      "        [186.4924, 186.4924, 186.4924, 186.4924, 186.4924, 186.4924],\n",
      "        [356.7704, 356.7704, 356.7704, 356.7704, 356.7704, 356.7704],\n",
      "        [356.2079, 356.2079, 356.2079, 356.2079, 356.2079, 356.2079],\n",
      "        [190.6295, 190.6295, 190.6295, 190.6295, 190.6295, 190.6295],\n",
      "        [452.6354, 452.6354, 452.6354, 452.6354, 452.6354, 452.6354],\n",
      "        [219.2616, 219.2616, 219.2616, 219.2616, 219.2616, 219.2616],\n",
      "        [474.5494, 474.5494, 474.5494, 474.5494, 474.5494, 474.5494],\n",
      "        [391.0132, 391.0132, 391.0132, 391.0132, 391.0132, 391.0132],\n",
      "        [202.4259, 202.4259, 202.4259, 202.4259, 202.4259, 202.4259],\n",
      "        [503.9469, 503.9469, 503.9469, 503.9469, 503.9469, 503.9469],\n",
      "        [155.3340, 155.3340, 155.3340, 155.3340, 155.3340, 155.3340],\n",
      "        [182.0125, 182.0125, 182.0125, 182.0125, 182.0125, 182.0125]])\n",
      "num_sum tensor(-5.4654e+21)\n",
      "numerator tensor(inf)\n",
      "denominator tensor([[inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf, inf]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/cwvjv87n3wq590ldrjtpds000000gn/T/ipykernel_48290/21031499.py:232: RuntimeWarning: overflow encountered in exp\n",
      "  numerator = self.policy_history[s, a, h, k] * np.exp(-1 * self.eta * num_sum)\n",
      "/var/folders/g_/cwvjv87n3wq590ldrjtpds000000gn/T/ipykernel_48290/21031499.py:240: RuntimeWarning: overflow encountered in exp\n",
      "  denominator += self.policy_history[s, a_prime, h, k] * np.exp(-1 * self.eta * inner_sum)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[25, 6]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[635], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDAPO_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[633], line 247\u001b[0m, in \u001b[0;36mDAPO.train_one_step\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdenominator\u001b[39m\u001b[38;5;124m'\u001b[39m, denominator)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m): \u001b[38;5;66;03m# do not update policy at last episode\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# self.policy_history[s][a][h][k + 1] = numerator / (denominator if denominator != 0 else 1)\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_history\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m numerator \u001b[38;5;241m/\u001b[39m denominator\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[25, 6]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "DAPO_test.train_one_step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1279, 0.0046, 0.0124, 0.0747, 0.0176, 0.0026, 0.0060, 0.0372, 0.0635,\n",
       "        0.0043, 0.0167, 0.1032, 0.0061, 0.0765, 0.0026, 0.0225, 0.0720, 0.0083,\n",
       "        0.0438, 0.0046, 0.0143, 0.0683, 0.0056, 0.1142, 0.0906])"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAPO_test.get_occupancy(99, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6750, 0.0008, 0.0008, 0.0853, 0.0008, 0.0008, 0.0008, 0.0008, 0.0847,\n",
       "        0.0008, 0.0008, 0.0009, 0.0008, 0.0008, 0.0008, 0.0008, 0.0201, 0.0008,\n",
       "        0.0015, 0.0008, 0.0008, 0.0035, 0.0008, 0.0842, 0.0315])"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAPO_test.get_one_step_transition(0, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 8],\n",
       "        [8, 8]])"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[2, 2], [2, 2]])\n",
    "torch.linalg.matrix_power(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(DAPO_test.get_one_step_transition(0, 0)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DAPO_test.get_one_step_transition(99, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.linalg.matrix_power(x, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834],\n",
       "        [0.1367, 0.0081, 0.0653, 0.0468, 0.0141, 0.0029, 0.0124, 0.0266, 0.0468,\n",
       "         0.0083, 0.0305, 0.1312, 0.0126, 0.0660, 0.0034, 0.0293, 0.0457, 0.0125,\n",
       "         0.0246, 0.0087, 0.0217, 0.0462, 0.0175, 0.0984, 0.0834]])"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.0000)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "y = torch.tensor([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 1, 2]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.repeat(1, 1, 2)[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.repe[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [1, 3]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0].repeat(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 7])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.7497e-01, 8.3333e-04, 8.3333e-04, 8.5338e-02, 8.3333e-04, 8.3333e-04,\n",
       "         8.3333e-04, 8.3397e-04, 8.4658e-02, 8.3333e-04, 8.3333e-04, 9.2366e-04,\n",
       "         8.3333e-04, 8.3341e-04, 8.3333e-04, 8.3333e-04, 2.0086e-02, 8.3333e-04,\n",
       "         1.4896e-03, 8.3333e-04, 8.3333e-04, 3.4823e-03, 8.3333e-04, 8.4229e-02,\n",
       "         3.1488e-02],\n",
       "        [6.6758e-03, 6.1377e-01, 6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.7485e-03, 6.6667e-03, 6.6667e-03, 9.1737e-02,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 1.0142e-02, 6.6679e-03, 6.6667e-03,\n",
       "         4.8768e-02, 6.6667e-03, 6.6667e-03, 9.6945e-02, 6.6667e-03, 1.1875e-02,\n",
       "         6.6667e-03],\n",
       "        [6.6828e-03, 6.6667e-03, 7.9442e-01, 6.6667e-03, 6.6667e-03, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03, 1.0736e-02,\n",
       "         6.6667e-03, 6.6673e-03, 6.6667e-03, 6.6667e-03, 1.9488e-02, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 9.2708e-03, 6.6667e-03, 3.2739e-02,\n",
       "         6.6667e-03],\n",
       "        [8.3069e-03, 8.3333e-04, 8.3333e-04, 3.2838e-01, 8.3333e-04, 8.3333e-04,\n",
       "         8.3333e-04, 8.3333e-04, 1.4495e-01, 8.3333e-04, 8.3333e-04, 1.4182e-01,\n",
       "         8.3333e-04, 8.4167e-02, 8.3333e-04, 8.3333e-04, 2.1846e-02, 8.3333e-04,\n",
       "         1.8432e-02, 8.3333e-04, 8.4167e-02, 5.6473e-02, 8.3333e-04, 9.8957e-02,\n",
       "         8.4012e-04],\n",
       "        [6.6820e-03, 6.6667e-03, 6.6667e-03, 6.6741e-03, 6.5473e-01, 6.6667e-03,\n",
       "         6.6667e-03, 7.9894e-03, 1.1875e-02, 6.6667e-03, 5.5287e-02, 9.0000e-02,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 1.1717e-02, 1.7106e-02, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 1.7083e-02, 6.6667e-03, 2.7522e-02,\n",
       "         6.6667e-03],\n",
       "        [7.5833e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 5.5000e-02, 3.4167e-02,\n",
       "         3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 1.1750e-01,\n",
       "         3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02,\n",
       "         3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02, 3.4167e-02,\n",
       "         3.4167e-02],\n",
       "        [6.7231e-03, 6.9922e-03, 6.6667e-03, 1.1956e-02, 6.6667e-03, 6.6667e-03,\n",
       "         7.7174e-01, 6.6667e-03, 6.6667e-03, 6.7218e-03, 6.6667e-03, 2.7500e-02,\n",
       "         6.6667e-03, 6.6830e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6775e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 4.8343e-02, 6.6667e-03, 6.6668e-03,\n",
       "         6.6667e-03],\n",
       "        [1.4733e-01, 6.6667e-03, 6.6667e-03, 9.2930e-02, 6.6959e-03, 6.6667e-03,\n",
       "         6.6667e-03, 4.7948e-01, 2.5921e-02, 6.6667e-03, 6.6667e-03, 6.7548e-03,\n",
       "         6.6667e-03, 5.4292e-02, 6.6667e-03, 6.6667e-03, 2.9817e-02, 6.6667e-03,\n",
       "         6.7390e-03, 6.6667e-03, 6.6667e-03, 4.9039e-02, 6.6667e-03, 6.6897e-03,\n",
       "         7.6464e-03],\n",
       "        [9.3578e-02, 3.3333e-03, 3.3333e-03, 9.9777e-03, 3.3333e-03, 3.3333e-03,\n",
       "         3.3333e-03, 4.6354e-03, 1.9335e-01, 3.3333e-03, 3.3333e-03, 8.1865e-02,\n",
       "         3.3333e-03, 3.7300e-03, 3.3333e-03, 3.3333e-03, 3.3558e-03, 3.3333e-03,\n",
       "         7.2780e-02, 3.3333e-03, 3.3333e-03, 1.0750e-01, 3.3333e-03, 2.5341e-01,\n",
       "         1.2915e-01],\n",
       "        [6.7205e-03, 6.6798e-03, 6.6798e-03, 1.3624e-02, 6.6798e-03, 6.6798e-03,\n",
       "         6.6798e-03, 6.6798e-03, 3.4458e-02, 6.5249e-01, 6.6798e-03, 9.2841e-03,\n",
       "         6.6798e-03, 6.2235e-02, 6.6798e-03, 6.6798e-03, 1.3709e-02, 6.6798e-03,\n",
       "         6.6798e-03, 6.6798e-03, 6.6798e-03, 6.6798e-03, 6.6798e-03, 9.2840e-03,\n",
       "         9.1315e-02],\n",
       "        [9.1471e-04, 8.3333e-04, 8.3333e-04, 8.3341e-04, 4.5455e-03, 8.3333e-04,\n",
       "         8.3333e-04, 4.3462e-03, 8.4789e-04, 8.3333e-04, 7.0098e-01, 1.1251e-02,\n",
       "         8.3333e-04, 8.3333e-04, 8.3333e-04, 5.3418e-02, 8.4167e-02, 8.3333e-04,\n",
       "         8.3333e-04, 8.3333e-04, 8.3333e-04, 8.3333e-04, 8.3333e-04, 4.2539e-02,\n",
       "         8.4495e-02],\n",
       "        [1.0692e-02, 5.2083e-05, 8.3385e-02, 1.6722e-02, 5.2083e-05, 5.2083e-05,\n",
       "         5.2083e-05, 5.2083e-05, 4.1729e-02, 5.2083e-05, 5.2084e-05, 7.4013e-01,\n",
       "         5.2083e-05, 4.8270e-02, 5.2083e-05, 7.0628e-04, 1.3548e-03, 6.2256e-05,\n",
       "         2.3523e-02, 5.2083e-05, 5.2083e-05, 2.1759e-04, 5.2083e-05, 2.1548e-02,\n",
       "         1.1033e-02],\n",
       "        [6.6667e-03, 2.7500e-02, 6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03,\n",
       "         6.6667e-03, 6.6672e-03, 6.9473e-03, 6.6667e-03, 6.6667e-03, 6.7390e-03,\n",
       "         7.7433e-01, 9.2709e-03, 6.6667e-03, 6.6667e-03, 4.8334e-02, 6.6667e-03,\n",
       "         6.6668e-03, 6.6667e-03, 6.6667e-03, 6.8750e-03, 6.6667e-03, 6.6667e-03,\n",
       "         6.6730e-03],\n",
       "        [6.9923e-03, 6.6667e-03, 6.6667e-03, 4.9373e-02, 6.6667e-03, 6.6667e-03,\n",
       "         6.6667e-03, 1.0928e-02, 1.7140e-02, 6.6667e-03, 6.6667e-03, 1.1896e-02,\n",
       "         6.6667e-03, 4.7128e-01, 6.6667e-03, 6.6667e-03, 2.7587e-02, 6.6667e-03,\n",
       "         4.8361e-02, 6.6667e-03, 6.6667e-03, 1.7095e-02, 6.6667e-03, 2.2312e-02,\n",
       "         2.2371e-01],\n",
       "        [3.1949e-02, 2.5005e-02, 2.5005e-02, 2.7088e-02, 2.5005e-02, 2.5005e-02,\n",
       "         2.5005e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02, 2.8477e-02,\n",
       "         2.5005e-02, 1.0834e-01, 1.5811e-01, 1.0834e-01, 2.5005e-02, 2.5005e-02,\n",
       "         6.6671e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02, 2.5005e-02,\n",
       "         4.5954e-02],\n",
       "        [2.7500e-02, 6.6667e-03, 6.6667e-03, 6.8294e-03, 5.4065e-02, 6.6667e-03,\n",
       "         6.6667e-03, 6.6668e-03, 1.3611e-02, 6.6667e-03, 1.7191e-02, 2.1478e-02,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 7.2431e-01, 6.6680e-03, 6.6667e-03,\n",
       "         6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6683e-03, 6.6667e-03, 7.7939e-03,\n",
       "         2.0556e-02],\n",
       "        [2.3092e-02, 8.3333e-04, 8.3333e-04, 1.7976e-02, 8.3333e-04, 8.3333e-04,\n",
       "         8.3333e-04, 4.2950e-02, 5.6942e-02, 8.3333e-04, 8.3333e-04, 3.4167e-02,\n",
       "         8.3333e-04, 7.7056e-03, 8.3333e-04, 4.2500e-02, 6.4362e-01, 8.3333e-04,\n",
       "         8.5210e-04, 8.3333e-04, 8.3333e-04, 2.7637e-02, 8.3333e-04, 9.0719e-02,\n",
       "         1.0012e-03],\n",
       "        [3.3740e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03,\n",
       "         3.3333e-03, 3.3333e-03, 2.4188e-02, 3.3333e-03, 3.3333e-03, 1.3203e-01,\n",
       "         3.3333e-03, 3.3337e-03, 3.3333e-03, 3.3333e-03, 3.5529e-03, 7.6930e-01,\n",
       "         3.4961e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03, 3.3333e-03, 3.9845e-03,\n",
       "         3.4159e-03],\n",
       "        [3.5401e-03, 3.3333e-03, 3.3333e-03, 1.6683e-01, 3.3333e-03, 3.3333e-03,\n",
       "         3.3333e-03, 3.3333e-03, 4.9617e-02, 3.3333e-03, 3.3333e-03, 2.2279e-02,\n",
       "         3.3333e-03, 1.1256e-02, 3.3333e-03, 3.4249e-03, 9.7101e-02, 3.3333e-03,\n",
       "         3.8351e-01, 3.3333e-03, 3.3333e-03, 3.3334e-03, 3.3333e-03, 1.1405e-01,\n",
       "         9.8391e-02],\n",
       "        [9.0462e-02, 6.6927e-03, 6.6927e-03, 7.3193e-03, 6.7010e-03, 6.6927e-03,\n",
       "         6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6927e-03, 9.0067e-02,\n",
       "         6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6931e-03, 6.6927e-03,\n",
       "         6.6927e-03, 6.7160e-01, 6.6927e-03, 6.6927e-03, 6.6927e-03, 6.6941e-03,\n",
       "         6.6927e-03],\n",
       "        [6.6667e-03, 6.6667e-03, 6.6667e-03, 6.6667e-03, 1.7083e-02, 6.6667e-03,\n",
       "         6.6667e-03, 6.6802e-03, 4.8333e-02, 6.6667e-03, 4.8333e-02, 6.6673e-03,\n",
       "         6.6667e-03, 2.7502e-02, 6.6667e-03, 6.6802e-03, 1.7083e-02, 6.9239e-03,\n",
       "         7.9710e-03, 6.6667e-03, 6.9192e-01, 6.6667e-03, 6.6667e-03, 6.6689e-03,\n",
       "         2.8157e-02],\n",
       "        [5.2399e-02, 3.3333e-03, 3.3333e-03, 5.8976e-02, 8.9092e-03, 3.3333e-03,\n",
       "         3.3333e-03, 9.6541e-03, 3.3874e-03, 3.3333e-03, 3.3333e-03, 9.7083e-02,\n",
       "         3.3333e-03, 3.3333e-03, 3.3333e-03, 3.3435e-03, 1.0494e-02, 3.3333e-03,\n",
       "         3.5371e-03, 3.3333e-03, 3.3333e-03, 6.6965e-01, 3.3333e-03, 3.5897e-02,\n",
       "         3.3334e-03],\n",
       "        [6.6699e-03, 6.6699e-03, 6.6699e-03, 6.6722e-03, 6.6699e-03, 6.6699e-03,\n",
       "         6.6699e-03, 6.6699e-03, 6.7106e-03, 6.6699e-03, 6.6699e-03, 6.6699e-03,\n",
       "         6.6699e-03, 6.6699e-03, 6.6699e-03, 6.6699e-03, 6.6708e-03, 6.6699e-03,\n",
       "         6.6699e-03, 6.6699e-03, 6.6699e-03, 6.6699e-03, 8.3988e-01, 6.6711e-03,\n",
       "         6.6699e-03],\n",
       "        [2.6601e-01, 5.2083e-05, 5.2083e-05, 8.5644e-03, 5.2083e-05, 5.2083e-05,\n",
       "         5.2083e-05, 8.5094e-02, 2.9622e-04, 5.2083e-05, 4.1719e-02, 3.9997e-02,\n",
       "         5.2083e-05, 1.3073e-02, 5.2083e-05, 1.3941e-02, 5.2084e-05, 5.2088e-05,\n",
       "         7.8451e-04, 5.2083e-05, 5.2083e-05, 1.1793e-04, 5.2083e-05, 5.2967e-01,\n",
       "         5.2085e-05],\n",
       "        [1.2519e-02, 1.6667e-03, 1.6667e-03, 1.4067e-02, 1.6667e-03, 1.6667e-03,\n",
       "         1.6667e-03, 2.3177e-03, 5.8325e-02, 1.6667e-03, 4.2708e-03, 5.6049e-02,\n",
       "         1.6667e-03, 2.1617e-01, 1.6667e-03, 1.6667e-03, 9.3584e-03, 1.6787e-03,\n",
       "         2.2682e-02, 1.6667e-03, 1.6667e-03, 1.6667e-03, 1.6667e-03, 2.9942e-03,\n",
       "         5.7790e-01]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1077, 0.0243, 0.0140, 0.0836, 0.0313, 0.0458, 0.0150, 0.0656, 0.0819,\n",
       "        0.0242, 0.0289, 0.0720, 0.0140, 0.0762, 0.0395, 0.0276, 0.0793, 0.0209,\n",
       "        0.0776, 0.0248, 0.0187, 0.0697, 0.0097, 0.0875, 0.0794])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAPO_test.get_occupancy(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAPO_test.play_episode(DAPO_test.policy_history[:,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAPO_test.get_occupancy(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 7, 8])\n",
    "y = torch.tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / (torch.maximum(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  8],\n",
       "        [ 5, 12]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2]) * torch.tensor([[3, 4], [5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_test = DAPO_test.play_episode(DAPO_test.policy_history[:, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4845.0845)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([ 0.0000, 64.3088, 54.7057, 64.8670, 58.7536, 52.9981, 64.9772, 12.2039,\n",
    "        64.9661, 52.9981, 64.9803, 14.1496, 14.1496, 64.9803, 52.9981, 64.9661,\n",
    "        64.3088, 27.7446, 64.3088, 65.0443, 65.1410, 14.1496, 14.1496, 55.8280,\n",
    "        64.8670, 64.8670, 62.6964, 64.5440, 65.1419, 62.6964, 64.8670, 64.9661,\n",
    "        58.7536, 65.0443, 27.7446, 14.1496,  0.0000, 64.8670, 63.8448, 64.9823,\n",
    "        64.9823, 64.5440, 64.3088, 64.9803,  0.0000, 64.3088, 64.5440, 64.8670,\n",
    "        14.1496, 14.1496, 39.7341, 65.1410, 64.3088, 65.1419, 63.8448, 14.1496,\n",
    "        65.1419, 65.0418, 64.9661, 54.7057, 27.7446, 55.8280, 55.8280,  0.0000,\n",
    "        64.9803, 65.0443, 12.2039, 64.5440, 63.8448, 65.0533, 54.7057, 65.0533,\n",
    "        64.3088, 63.8448, 64.8670, 53.6274, 58.7536, 64.9661, 65.0173,  0.0000,\n",
    "        65.1419, 65.0173, 58.7536, 64.5440, 65.0533, 64.9823, 63.8448, 65.1419,\n",
    "        55.8280,  0.0000, 52.9981, 52.9981, 14.1496, 55.8280, 63.8448, 65.0533,\n",
    "        54.7057, 12.2039, 62.6964,  0.0000])\n",
    "\n",
    "torch.sum(x[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAPO_test.policy_history[:, :, 99, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "s_p = torch.tensor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat184-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
